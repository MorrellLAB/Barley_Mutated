#!/bin/bash

#   More complete information on how to fill out
#       this Config file can be found at:
#       https://github.com/MorrellLAB/sequence_handling/wiki/Configuration

###################################################
##########     Shared across handlers    ##########
###################################################

#   Where are we storing the output files?
#       Final directory is ${OUT_DIR}/Name_of_Handler
OUT_DIR=/panfs/roc/groups/9/morrellp/shared/Datasets/Alignments/mutated_8_morex_v3
#   Store some handler outputs in scratch due to shared space storage shortage
#OUT_DIR=/scratch.global/liux1299/mutated_8_morex

#   Name this project
PROJECT=mutated_8_barley

#   What email should we use for job notifications?
EMAIL=liux1299@umn.edu

#   What encoding is used for quality values?
#       Look at the FastQC files to determine the sequence encoding.
#       Choose from: 'sanger', 'illumina', 'solexa', or 'phred'
#           Illumina 1.8+ and Oxford Nanopore use 'sanger' encoding.
QUAL_ENCODING=sanger

#   Sequencing platform technology
#       What platform were the reads produced?
#       Valid options are:
#           CAPILLARY, LS454, ILLUMINA,
#           SOLID, HELICOS, IONTORRENT,
#           ONT, and PACBIO
SEQ_PLATFORM=ILLUMINA

#   What reference genome are we using?
#       Include the full physical file path.
REF_GEN=/panfs/roc/groups/9/morrellp/shared/References/Reference_Sequences/Barley/Morex_v3/Barley_MorexV3_pseudomolecules_parts.fasta

#   Is this organism barley?
#       Choose from: "true" or "false"
BARLEY=true

#   Are you running the analysis on the Minnesota Supercomputing Institute (MSI)?
#       Choose from: "true" or "false"
MSI=true

#   Are you submitting the job with qsub from PBS (Portable Batch System)
#       Choose from: "true" or "false"
USE_PBS=false

#   Are you submitting the job with sbatch from Slurm?
#       Choose from: "true" or "false"
USE_SLURM=true

#   Do the quality scores need to be adjusted for GATK? Default: false
#       Change to true if you get errors from GATK like:
#       "<Sample> appears to be using the wrong encoding for quality scores: we encountered an extremely high quality score"
FIX_QUALITY_SCORES=false

#   Where should Picard/GATK store temporary files?
#       If you've encountered issues with running out of temp space
#           with picard, you can optionally specify a temp directory
#       Otherwise, leave blank
TMP=/scratch.global/liux1299/mutated-temp

############################################
##########   Quality_Assessment   ##########
############################################

#   Which queue are we submitting the job to?
#   Note: For USE_PBS, we can only specify one queue
#         For USE_SLURM, we can specify multiple queues that are comma separated, no spaces (e.g., "small,ram256g,ram1t")
QA_QUEUE="small,ram256g,ram1t"

#   USE_SLURM: What are our sbatch settings?
#       Below are the recommended settings
QA_SBATCH="--nodes=1 --ntasks-per-node=4 --mem=12gb -t 12:00:00 --mail-type=ALL --mail-user=${EMAIL} -p ${QA_QUEUE}"

#   Provide a list of FastQ, SAM, or BAM files to be used
#       Include the full file path.
QA_SAMPLES=/scratch.global/liux1299/mutated_8_morex/Adapter_Trimming/mut8lines_trimmed_list.txt

#   What is the size of the genome in basepairs (for whole-genome sequencing)
#   or capture region (for exome capture)?
#   If unavailable, put "NA"
TARGET="NA"

############################################
##########    Adapter_Trimming    ##########
############################################

#   Which queue are we submitting the job to?
#   Note: For USE_PBS, we can only specify one queue
#         For USE_SLURM, we can specify multiple queues that are comma separated, no spaces (e.g., "small,ram256g,ram1t")
AT_QUEUE="small,ram256g,ram1t"

#   USE_SLURM: What are our sbatch settings?
#       Below are the recommended settings
AT_SBATCH="--nodes=1 --ntasks-per-node=6 --mem=2gb --tmp=1gb -t 12:00:00 --mail-type=ALL --mail-user=${EMAIL} -p ${AT_QUEUE}"

#   Where is the list of original FastQ files?
#       Include the full file path.
RAW_SAMPLES=/panfs/roc/groups/9/morrellp/shared/Datasets/NGS/Barley_WGS/8_mutated_lines/mutated_8_lines_fastq_list.txt

#   What shared suffix do the forward samples have?
#       Example: _1_sequence.txt.gz
FORWARD_NAMING=_R1.fastq.gz

#   What shared suffix do the reverse samples have?
#       Example: _2_sequence.txt.gz
REVERSE_NAMING=_R2.fastq.gz

#   If you have single-end samples, leave
#       FORWARD_NAMING and REVERSE_NAMING
#       filled with values that do NOT
#       match your samples

#	  Where is the adapter file? Include the full physical file path.
ADAPTERS=/panfs/roc/groups/9/morrellp/shared/References/Adapters/Nextera_adapter_trimming.fa
#ADAPTERS=/panfs/roc/groups/9/morrellp/shared/References/Adapters/Nextera.fa

#   What is Scythe's prior contamination rate?
#       Scythe's documentation suggests starting at
#       0.05 and then experimenting as needed
PRIOR=0.05

############################################
##########  	Read_Mapping	  ##########
############################################

#   Which queue are we submitting the job to?
#   Note: For USE_PBS, we can only specify one queue
#         For USE_SLURM, we can specify multiple queues that are comma separated, no spaces (e.g., "small,ram256g,ram1t")
RM_QUEUE="small,ram256g,ram1t"

#   USE_SLURM: What are our sbatch settings?
#       Below are the recommended settings
RM_SBATCH="--nodes=1 --ntasks-per-node=16 --mem=42gb --tmp=12gb -t 36:00:00 --mail-type=ALL --mail-user=${EMAIL} -p ${RM_QUEUE}"

#   Where is our list of trimmed samples?
#       Include the full file path.
TRIMMED_LIST=/scratch.global/liux1299/mutated_8_morex/Adapter_Trimming/mut8lines_trimmed_list.txt

#   How are trimmed forward samples named?
#       If using the Adapter_Trimming handler
#       FORWARD_TRIMMED=_Forward_ScytheTrimmed.fastq.gz
#       If using the Quality_Trimming handler
#       FORWARD_TRIMMED=_R1_trimmed.fastq.gz
FORWARD_TRIMMED=_Forward_ScytheTrimmed.fastq.gz

#   How are trimmed reverse samples named?
#       If using the Adapter_Trimming handler
#       REVERSE_TRIMMED=_Reverse_ScytheTrimmed.fastq.gz
#       If using the Quality_Trimming handler
#       REVERSE_TRIMMED=_R2_trimmed.fastq.gz
REVERSE_TRIMMED=_Reverse_ScytheTrimmed.fastq.gz

#   How are trimmed single-end samples named?
#       If using the Adapter_Trimming handler
#       SINGLES_TRIMMED=_Single_ScytheTrimmed.fastq.gz
#       If using the Quality_Trimming handler
#       SINGLES_TRIMMED=_single_trimmed.fastq.gz
SINGLES_TRIMMED=_Single_ScytheTrimmed.fastq.gz

#   BWA mem parameters; below are the defaults for BWA mem
#       Note that you may need to adjust parameters based on species
#       How many threads are we using?
THREADS=16

#       What is our minimum seed length?
SEED=8

#       What is the band width?
WIDTH=100

#       What is our off-diagonal x-dropoff (Z-dropoff)?
DROPOFF=100

#       What is our re-seed value?
RE_SEED=1.0

#       What is our cutoff value?
CUTOFF=10000

#       What is our matching score?
MATCH=1

#       What is our mismatch penalty?
MISMATCH=4

#       What is our gap penalty?
GAP=8

#       What is our gap extension penalty?
EXTENSION=1

#       What is our clipping penalty?
CLIP=5

#       What is our unpaired read penatly?
UNPAIRED=9

#       Do we rescue missing hits? Note, this means that reads may not be matched. Requires paired-end mode
RESCUE=false

#       Do we assume the first input query is interleaved?
INTERLEAVED=false

#       What is our minimum threshold?
RM_THRESHOLD=125

#       Do we output all alignments, and mark as secondary?
SECONDARY=false

#       Do we append FASTA/Q comments to SAM files?
APPEND=false

#       Do we use hard clipping?
HARD=false

#       Do we mark split hits as secondary?
SPLIT=true

#       What is the verbosity level?
#           Choose from:
#               'disabled', 'errors'
#               'warnings', 'all', or 'debug'
VERBOSITY='all'

############################################
##########     SAM_Processing     ##########
############################################

#   Which queue are we submitting the job to?
#   Note: For USE_PBS, we can only specify one queue
#         For USE_SLURM, we can specify multiple queues that are comma separated, no spaces (e.g., "small,ram256g,ram1t")
SP_QUEUE="ram1t,amd2tb"

#   USE_SLURM: What are our sbatch settings?
#       Below are the recommended settings
SP_SBATCH="--nodes=1 --ntasks-per-node=12 --mem=800gb --tmp=150gb -t 24:00:00 --mail-type=ALL --mail-user=${EMAIL} -p ${SP_QUEUE}"

#   Specify how you would like your SAM files to be processed
#       Choose from:
#           'picard' (recommended) or 'samtools'
METHOD='picard'

#   Number of threads used for SAM_Processing
#   Using more threads requires more memory
#       This is ignored when USE_PBS=true
SAM_PROCESSING_THREADS=12

#   Where is the list of read-mapped samples?
#       To generate this list, use sample_list_generator.sh
#       located at /sequence_handling/HelperScripts/sample_list_generator.sh
MAPPED_LIST=/scratch.global/liux1299/mutated_8_morex/Read_Mapping/sam_list.txt

#   The next two variables are only used if processing with Picard
#       If using SAMtools, leave these variables blank

#   What is the maximum number of file handles that we can use?
#       For UNIX systems, the per-process maximum number of files
#           that can be open may be found with 'ulimit -n'
#       We recommend setting MAX_FILES to be slightly under this value
MAX_FILES=1000

#   What is the maximum number (integer) of records stored in RAM before spilling to disk?
#       This is used in the Picard SortSam and MarkDuplicates and is ignored if SAM_Processing with Samtools.
#       You can try reducing this number if you are having core dump/out of memory errors.
#       Increasing this number reduces the number of file handles needed to sort the file,
#       and increases the amount of RAM needed.
#   Default value: 500000
PICARD_MAX_REC_IN_RAM=50000

#   What is the sorting collection size ratio we want to use?
#       This is used in Picard MarkDuplicates and is ignored if SAM_Processing with Samtools.
#       You can try reducing this number if you are having core dump/out of memory errors.
#   Default value: 0.25
SORTING_COLL_SIZE_RATIO=0.10

############################################
##########    Haplotype_Caller    ##########
############################################

#   Which queue are we submitting the job to?
#   Note: For USE_PBS, we can only specify one queue
#         For USE_SLURM, we can specify multiple partitions that are comma separated, no spaces (e.g., "small,ram256g,ram1t")
HC_QUEUE="ram256g,ram1t,amd2tb"

#   USE_SLURM: What are our sbatch settings?
#       Below are the recommended settings
HC_SBATCH="--nodes=1 --ntasks-per-node=4 --mem=240gb --tmp=100gb -t 24:00:00 --mail-type=ALL --mail-user=${EMAIL} -p ${HC_QUEUE}"

#   Number of threads used for Haplotype_Caller
#   Using more threads requires more memory
#   This is ignored when USE_PBS=true
#   Default: "NA"
HAPLOTYPE_CALLER_THREADS="NA"

#   Where is the list of finished BAM files?
#       To generate this list, use sample_list_generator.sh
#       located at /sequence_handling/HelperScripts/sample_list_generator.sh
FINISHED_BAM_LIST=/scratch.global/liux1299/mutated_8_morex/SAM_Processing/Picard/mut8morex_bam_list.txt

#   What is the nucleotide diversity per base pair (Watterson's theta)?
#       For barley: 0.008
#       For soybean: 0.001
THETA=0.008

#   If true, GATK will not trim down the active region from the full region
#       (active + extension) to just the active interval for genotyping
#       Recommended value: false
DO_NOT_TRIM_ACTIVE_REGIONS=false

#   If true, all bases will be considered active regions
#       Recommended value: false
FORCE_ACTIVE=false

#   Optional: Provide a list of custom intervals if you are interested in specific regions AND/OR want to parallelize processing across regions/chromosomes.
#   Default: false. Otherwise, fill in full path to intervals file.
#   IMPORTANT: GATK accepts the following file extensions:
#       .intervals, .list, .interval_list, .bed, and .vcf
#       sequence_handling Haplotype_Caller currently handles: .intervals, .list
#   For .intervals/.list files, format one per line in one of the two formats: 1) chr1 or 2) chr1:100-200
#   For example files, see: https://github.com/MorrellLAB/sequence_handling/tree/master/Example_Files
#       For additional info, please see Wiki page.
HC_CUSTOM_INTERVALS=false

#   Optional: If your genome has scaffolds or parts of the reference not covered by
#       the chromosomes above, include the full filepath to a SORTED list of
#       names. One scaffold name per line.
#   IMPORTANT: If you provide a scaffolds list, you must provide a custom intervals list too.
#   Default is set to false. Otherwise, fill in full path to intervals file.
HC_SCAFFOLDS=false

#   Do we want to parallelize across regions?
#   Set to true if yes.
#   Default is set to false.
#       Parallelizing across regions for Haplotype_Caller means each
#       region/chromosome provided in HC_CUSTOM_INTERVALS list gets run
#       in as it's own job for every sample.
HC_PARALLELIZE=false

################################################
##########     Genomics_DB_Import     ##########
################################################

#   USAGE NOTE: Genomics DB Import is used in GATK4.

#   Which queue are we submitting the job to?
#   Note: For USE_PBS, we can only specify one queue
#         For USE_SLURM, we can specify multiple queues that are comma separated, no spaces (e.g., "small,ram256g,ram1t")
#GDBI_QUEUE="ram256g,ram1t,amd2tb"
GDBI_QUEUE="max"

#   USE_SLURM: What are our sbatch settings?
#       Below are the recommended settings
#GDBI_SBATCH="--nodes=1 --ntasks-per-node=4 --mem=110gb --tmp=60gb -t 90:00:00 --mail-type=ALL --mail-user=${EMAIL} -p ${GDBI_QUEUE}"
# A few array indices needed more than 90 hours walltime: 3,5,7,9
GDBI_SBATCH="--nodes=1 --ntasks-per-node=4 --mem=56gb --tmp=40gb -t 100:00:00 --mail-type=ALL --mail-user=${EMAIL} -p ${GDBI_QUEUE}"

#   Number of threads used for Genomics_DB_IMPORT
#   Using more threads requires more memory
#       This is ignored when USE_PBS=true
GDBI_THREADS=4

#   REQUIRED: Please fill out variables under "Genotype_GVCFs" listed below.
#       Genomics_DB_Import shares variables with Genotype_GVCFs.

############################################
##########     Genotype_GVCFs     ##########
############################################

#   USAGE NOTE: For accurate results, run Genotype_GVCFs on all of your samples at once
#       DO NOT break this job into batches of samples

#   Which queue are we submitting the job to?
#   Note: For USE_PBS, we can only specify one queue
#         For USE_SLURM, we can specify multiple queues that are comma separated, no spaces (e.g., "small,ram256g,ram1t")
GG_QUEUE="ram256g,ram1t,amd2tb"

#   USE_SLURM: What are our sbatch settings?
#       Below are the recommended settings
GG_SBATCH="--nodes=1 --ntasks-per-node=4 --mem=80gb --tmp=60gb -t 48:00:00 --mail-type=ALL --mail-user=${EMAIL} -p ${GG_QUEUE}"

#   Number of threads used for Genotype_GVCFs
#   Using more threads requires more memory
#       This is ignored when USE_PBS=true
GG_THREADS=4

#   Where is the list of GVCF files?
#       To generate this list, use sample_list_generator.sh
#       located at /sequence_handling/HelperScripts/sample_list_generator.sh
GVCF_LIST=/scratch.global/liux1299/mutated_8_morex/Haplotype_Caller/gvcf_list_mut8morex.txt

#   What is the nucleotide diversity per base pair (Watterson's theta)?
#   Genotype_GVCFs uses the THETA under Haplotype_Caller for this

#   Where is the reference dictionary file? (.dict)
#       Include the full file path.
REF_DICT=/panfs/roc/groups/9/morrellp/shared/References/Reference_Sequences/Barley/Morex_v3/Barley_MorexV3_pseudomolecules_parts.dict

#   How many chromosomes or chromosome parts does the reference have, excluding scaffolds? (integer value)
#       For barley: 15 (7*2 chromosome parts + chrUn)
#       For soybean: 20
NUM_CHR=15

#   Optional: Provide a list of custom intervals if you are interested in specific regions AND/OR want to parallelize processing across regions/chromosomes.
#   IMPORTANT: GATK accepts the following file extensions:
#       .intervals, .list, .interval_list, .bed, and .vcf
#       sequence_handling currently handles: .intervals, .list, and .bed
#   For .intervals/.list files, format one per line as follows: chr1:100-200
#   For example files, see: https://github.com/MorrellLAB/sequence_handling/tree/master/Example_Files
#       For additional info, please see Wiki page.
#   Default is set to false. Otherwise, fill in full path to intervals file.
#   NOTE: If HC_PARALLELIZE=true and HC_CUSTOM_INTERVALS is set, the value below is ignored (uses HC_CUSTOM_INTERVALS).
CUSTOM_INTERVALS="/scratch.global/liux1299/mutated_8_morex/barley_chromosomes_parts.list"

#   Optional: If your genome has scaffolds or parts of the reference not covered by
#       the chromosomes above, include the full filepath to a SORTED list of
#       names. One scaffold name per line.
#   Default is set to false. Otherwise, fill in full path to intervals file.
#   NOTE: If HC_PARALLELIZE=true and HC_CUSTOM_INTERVALS is set, the value below is ignored (uses HC_SCAFFOLDS).
SCAFFOLDS=false

#   Do we want to parallelize across regions or chromosomes?
#   Set to true if yes.
#   Default is set to false.
#       Parallelizing across regions means each region/chromosome provided in
#       CUSTOM_INTERVALS list gets run as its own job.
#       In the case that we are parallelizing across regions AND we have scaffolds,
#       regions in CUSTOM_INTERVALS will be parallelized but the scaffolds will not be parallelized.
#       Scaffolds will instead be imported into its own gendb workspace.
#   NOTE: If HC_PARALLELIZE=true and HC_CUSTOM_INTERVALS is set, the value below is ignored.
PARALLELIZE=true

#  When parallelized, separate vcf output files are created for separate intervals in
#    ${OUT_DIR}/Genotype_GVCFs/vcf_split_regions
#  If you set the following option to true, these separate files get combined into a single vcf:
#     ${OUT_DIR}/Genotype_GVCFs/raw_variants.vcf
#  This option is ignored if PARALLELIZE=false
#     NOTE: this could require lots of memory, so if it fails, you need to increase
#           mem= option in GG_QSUB (this option is used even if you are not using qsub)
#  IMPORTANT: Currently, this option only works for non-PBS. For PBS users, split VCF files will
#     get combined and sorted in the Create_HC_Subset handler. If you don't plan to use the
#     Create_HC_Subset handler, the HelperScripts/combine_and_sort_split_vcf.sh
#     script will combine and sort the split VCF files for you.
#  Default is set to true.
GG_COMBINED_VCF=false

#   What is the sample ploidy? (integer value)
#   For highly inbred samples (most barleys): 1
PLOIDY=1

############################################
##########      Dependencies      ##########
############################################

#   This section defines installations to
#       various dependencies for sequence_handling.
#   If you are using the Minnesota Supercomputing Institute cluster
#       then uncomment all 'module load' lines.
#       Make sure you have access to all '_ML' modules.
#   If you need to install a dependency from source,
#       then uncomment the lines for the dependency and the
#       'export PATH=', and write the full path to the executable
#       for the program. For example:
#       PARALLEL=${HOME}/software/parallel-20151122/bin/parallel
#   If you have a system-wide installation for a program, you can
#       leave all lines commented out. sequence_handling will find
#       system-wide installed programs automatically.

#   Please visit https://github.com/MorrellLab/sequence_handling/wiki/Dependencies
#       for information on version requirements and compatibility

if [[ "$MSI" == "true" ]] ; then
#   Do we have GNU parallel installed
module load parallel/20190122
#PARALLEL=
#export PATH=${PARALLEL}:${PATH}

#   Do we have the Fastx Toolkit installed?
module load fastx_toolkit/0.0.14
#FASTX_TOOLKIT=
#export PATH=${FASTX_TOOLKIT}:${PATH}

#   Do we have FastQC installed?
module load fastqc/0.11.8
#FASTQC=
#export PATH=${FASTQC}:${PATH}

#   Do we have Riss-util installed?
module load riss_util/1.0
#RISS_UTIL=
#export PATH=${RISS_UTIL}:${PATH}

#   Do we have Seqqs installed?
module load seqqs_ML/3d05375
#SEQQS=
#export PATH=${SEQQS}:${PATH}

#   Do we have Sickle installed?
module load sickle_ML/1.33
#SICKLE=
#export PATH=${SICKLE}:${PATH}

#   Do we have Scythe installed?
module load scythe_ML/0.994
#SCYTHE=
#export PATH=${SCYTHE}:${PATH}

#   Do we have R installed?
module load R/3.6.3
#R=
#export PATH=${R}:${PATH}
#   REQUIRED if running on MSI: Where are the local R libraries stored?
#   To figure out the path, run the following interactively on MSI:
#       module load R/3.6.0
#       R # Start up R session on MSI
#       Sys.getenv()
#   Now, look for R_LIBS_USER and paste the path below
R_LIBS_USER="~/R/x86_64-pc-linux-gnu-library/3.6"

#   Do we have BWA installed?
module load bwa/0.7.17
#BWA=
#export PATH=${BWA}:${PATH}

#   Do we have SAMTools installed?
module load samtools/1.9
#SAMTOOLS=
#export PATH=${SAMTOOLS}:${PATH}

#   Do we have BEDTools 2.17.0 installed?
module load bedtools/2.17.0
#BEDTOOLS=
#export PATH=${BEDTOOLS}:${PATH}

#   Do we have htslib 1.9 installed?
#HTSLIB=
#export PATH=${HTSLIB}:${PATH}
module load htslib/1.9

#   Do we have Freebayes 1.3.1 from commit on 20190710 installed?
#FREEBAYES=
#export PATH=${FREEBAYES}:${PATH}
module load freebayes_ML/1.3.1_20190710

#   Do we have Java installed?
#module load java/jdk1.8.0_45
#JAVA=
#export PATH=${JAVA}:${PATH}
module load java/openjdk-8_202

#   What is the full file path for the GATK jar file?
#   You need BOTH the jar file and the module
GATK_JAR=/panfs/roc/groups/9/morrellp/public/Software/GATK_ML_4.1.2/gatk
module load gatk/4.1.2

#   What is the full file path for the Picard jar file?
PICARD_JAR=/panfs/roc/groups/9/morrellp/public/Software/picard_ML_2.23.1/picard.jar

#   Do we have vcftools installed?
module load vcftools_ML/0.1.14
#VCFTOOLS=
#export PATH=${VCFTOOLS}:${PATH}

#   Do we have vcflib installed?
module load vcflib_ML/1.0.0_rc2
#VCFLIB=
#export PATH=${VCFLIB}:${PATH}

#   Do we have python 3 installed?
module load python3/3.7.1_anaconda
#PYTHON3=
#export PATH=${PYTHON3}:${PATH}

#   Do we have analysis installed?
module load analysis/0.8.2
#ANALYSIS=
#export PATH=${ANALYSIS}:${PATH}

#   Do we have bcftools installed?
module load bcftools/1.10.2
#BCFTOOLS=
#export PATH=${BCFTOOLS}:${PATH}
#   Issue specific to MSI modules as of Mar 1, 2021
#   Comment out line if not needed.
#       When loading the bcftools/1.10.2 module, the perl/modules.centos7.5.26.1 module also
#       gets loaded. When the perl/modules.centos7.5.26.1 module gets loaded, the module
#       R/3.4.4-tiff also gets loaded. This introduces some R version conflicts since we are
#       loading R/3.6.3 above. A temporary workaround is to unload R/3.4.4-tiff
module unload R/3.4.4-tiff

#   Do we have python-epd installed?
#module load python-epd/1.5.2
#PYTHON-EPD=
#export PATH=${PYTHON-EPD}:${PATH}
# module load python3_ML/2.7.13

#   Do we have texlive installed?
module load texlive/20131202
#TEXLIVE=
#export PATH=${TEXLIVE}:${PATH}
fi
